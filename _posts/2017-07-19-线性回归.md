
# 线性回归

## 线性回归方程

当我们要预测的值是**连续**的时候，我们就称这个学习问题是一个**回归问题(regression problem)**，而当我们要预测的值是一个**离散值**时，我们就成为**分类问题(classification problem)**  
而在回归问题中最简单的模型就是线性回归模型  
例如下面这个例子，关于房子价格和面积大小以及卧室数量的关系  
![](https://aswz.github.io/assets/img/线性回归/线性回归例子.png)  
可以用该方程来近似表示价格**y**与特征**x<sub>1</sub>，x<sub>2</sub>**的关系  
![](https://aswz.github.io/assets/img/线性回归/线性回归方程.gif)  
其中**θ**为参数，而我们要求的这是这个**θ**，将其一般化得到下面方程  
![](https://aswz.github.io/assets/img/线性回归/线性回归方程一般化.gif)  
其中**θ**和**x**分别为**θ**和**x**组成的**向量**，在**x**中第一项为1  

## 成本函数

为了使**h(x)**与**y**尽可能的接近，我们需要一个函数用于描述**h(x)**与**y**之间的差距，或说误差，当函数值越小，h(x)就会越接近y值，这个函数，我们称之为**成本函数**  
![](https://aswz.github.io/assets/img/线性回归/成本函数.gif)  
这个式子之所以使用平方，是为了避免正负误差相互抵消，前面的1/2则是为了方便我们之后的计算，而我们要计算出**θ**的最小值，其实就是求成本函数的最小值时，**θ**的值  

## 梯度下降

**梯度**就是函数变化率最大方向的**向量**，而**梯度下降法**就是沿着这个方向的反方向减小，以找到最小值，形象来说就像在山顶上环视一周找到一个最陡峭的方向下山  
![](https://aswz.github.io/assets/img/线性回归/梯度下降.png)  
具体的算法和推导公式如下  
![](https://aswz.github.io/assets/img/线性回归/梯度下降算法.jpg)  
这里面的**α**是**学习速度**，是我们手动设置的参数，这种参数也称为**超参数**，它控制着我们每下降一次的步长，设置过小的话会导致收敛过慢，设置过大的话可能会幅度过大跳过最小值  
在单一样本中，就有下面的更新过程  
![](https://aswz.github.io/assets/img/线性回归/最小均方更新法.png)  
这个过程被称为**最小均方更新法(LMS)**，我们重复更新**θ**值，直到**J(θ)**的变化变得非常小或者收敛，从而获得我们的回归方程  

### 批梯度下降算法

就是将梯度下降用到**m**个样本中去，有下面的更新方法  
![](https://aswz.github.io/assets/img/线性回归/批梯度下降算法.jpg)  
这个计算方式被称为**批梯度下降算法(batch gradient descent)**，就是每次更新**θ**都要经过全部样本计算，求出当前点的**最陡的方向**，而这个方向也是这个点**全局下降的方向**，优点是可以比较精确的找到最小值，缺点是当**m**很大的时候，就会发现计算量也会变得非常巨大  

### 随机梯度下降算法

**随机梯度下降算法**计算方式如下  
![](https://aswz.github.io/assets/img/线性回归/随机梯度下降.png)  
在这个算法中每次都只取**一个样本**计算梯度，可以说是一种以更加大胆的下降方式，从**1到m**是为了充分利用样本，不造成浪费，由于**随机梯度下降**每次只取一个样本，所以收敛的速度会比**批梯度下降**快的多，但是它每次下降的方向不一定就是最优解的方向，有时候甚至会走反方向，所以随机梯度下降可能永远都不会收敛于最小值点，而实际中我们只用它在最小值附近即可  

## 正规方程组

除了使用**梯度下降**的方法，还可以用**正规方程组**的方法求得最小值  
在推导前需要知道几个矩阵求导的性质  
![](https://aswz.github.io/assets/img/线性回归/矩阵求导性质.png)  
![](https://aswz.github.io/assets/img/线性回归/正规方程组推导.png)  
最后可以得到  
![](https://aswz.github.io/assets/img/线性回归/正规方程组求参数.gif)  
    

## 局部加权回归

**局部加权回归(LWR,locally weighted linear regression)**，通常情况下，线性回归不能很好的拟合数据，可能要用更复杂的特征去拟合，也可以用**局部加权回归**这种**无参数方法**  
**成本函数**变为下面式子  
![](https://aswz.github.io/assets/img/线性回归/局部加权回归.png)  
其中**ω<sup>(i)</sup>**为  
![](https://aswz.github.io/assets/img/线性回归/指数衰减函数.png)  
函数中的**τ**称作**带宽（bandwidth）**（或**波长**）参数，它控制了权值随距离下降的速率，如果**τ**值较**小**，就会得到一个较窄的钟形曲线，这意味着随着离给定查询点**x**的**距离越大**，训练样本**x(i)**的**权值下降的速度就越快**  
如果**τ**值较**大**，就会得到一个较宽的钟形曲线，这意味着随着离给定查询点**x**的**距离变大**时，训练样本**x(i)**的**权值下降的速度会相对较慢**  
不过这种算法对于**每次查询**都需要根据整个训练集做出拟合，如果查询的频率很高，就很耗费大量时间  


