---
layout:     post
title:      "Logistic回归"
subtitle:   ""
date:       2017-07-19 18:00:00
author:     "Wes"
header-img: "img/Logistic回归-bg.png"
catalog: true
tags:
    - Machine Learning
---

# Logistic回归

Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：  
![](https://aswz.github.io/assets/img/Logistic回归/logistic函数.jpg)  
它的函数图像如下：  
![](https://aswz.github.io/assets/img/Logistic回归/Logistic回归图像.png)  
**对于这个logistic函数求导有个特别的性质**  
![](https://aswz.github.io/assets/img/Logistic回归/logistic函数求导性质.png)  
这个函数是在**(0,1)**之间取值的，当**z->∞**时，**g(z)**趋向1，当**z->-∞**时，**g(z)**趋向0  
和之前一样可以用求取极大似然值方法来求的参数的最优值  
先给我们的分类模型赋予一些列概率假设  
![](https://aswz.github.io/assets/img/Logistic回归/概率推导.jpg)  
我们也可以将这两个假设合入一个简洁的式子  
![](https://aswz.github.io/assets/img/Logistic回归/概率推导合并.jpg)  
然后我们取似然函数  
![](https://aswz.github.io/assets/img/Logistic回归/logistic回归似然值函数.png)  
![](https://aswz.github.io/assets/img/Logistic回归/logistic回归似然值对数函数.png)  
然后为了取得最大值，我们可以和之前类似的采用**梯度上升**的方法  
![](https://aswz.github.io/assets/img/Logistic回归/梯度上升.gif)  
然后我们对**l(θ)**求导，可得  
![](https://aswz.github.io/assets/img/Logistic回归/logistic回归似然值对数函数求导.png)  
最后应用到随机梯度上升  
![](https://aswz.github.io/assets/img/Logistic回归/随机梯度上升.gif)  
和批随机梯度上升  
![](https://aswz.github.io/assets/img/Logistic回归/批梯度上升.gif)  
将他们和之前的最小均方法里面的梯度下降的θ的更新公式作比较，会发现二者一模一样，但是，事实上这里的**h<sub>θ</sub>(x)**已经变为关于**θ<sup>T</sup>x(i)**的非线性函数（它的函数式和线性回归的**h(x)**是不一样的），所以这与我们的线性回归并不是同一个算法  